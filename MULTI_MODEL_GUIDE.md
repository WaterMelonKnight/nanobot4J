# Nanobot4J - å¤šæ¨¡å‹æ¶æ„é‡æ„å®Œæˆ

## ğŸ“‹ æ¶æ„æ¦‚è§ˆ

æˆ‘å·²ç»ä¸ºä½ å®Œæˆäº† Nanobot4J çš„å¤šæ¨¡å‹å¹¶å‘é…ç½®é‡æ„ã€‚æ–°æ¶æ„æ”¯æŒåŒæ—¶é…ç½®å’Œä½¿ç”¨å¤šä¸ª LLM æä¾›å•†ï¼ˆDeepSeekã€Kimiã€Ollama ç­‰ï¼‰ã€‚

## ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

### 1. é…ç½®å±‚ (Configuration Layer)

#### `MultiModelProperties.java`
- ä½¿ç”¨ `@ConfigurationProperties` ç»‘å®šé…ç½®
- æ”¯æŒé…ç½®å¤šä¸ªæ¨¡å‹æä¾›å•†
- æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹é…ç½®ï¼šAPI Keyã€Base URLã€æ¸©åº¦ã€è¶…æ—¶ç­‰
- æ”¯æŒé™çº§ç­–ç•¥é…ç½®

#### `application.yml`
```yaml
nanobot:
  llm:
    default-model: ollama
    models:
      deepseek:
        enabled: true
        provider: openai-compatible
        base-url: https://api.deepseek.com/v1
        api-key: ${DEEPSEEK_API_KEY}
        model: deepseek-chat
      kimi:
        enabled: true
        provider: openai-compatible
        base-url: https://api.moonshot.cn/v1
        api-key: ${KIMI_API_KEY}
        model: moonshot-v1-8k
      ollama:
        enabled: true
        provider: ollama
        base-url: http://localhost:11434
        model: bitnet
    fallback:
      enabled: true
      order: [ollama, deepseek, kimi]
```

### 2. å·¥å‚å±‚ (Factory Layer)

#### `ModelProviderFactory.java`
- **å·¥å‚æ¨¡å¼ + ç­–ç•¥æ¨¡å¼**
- æ ¹æ®é…ç½®åŠ¨æ€åˆ›å»ºä¸åŒçš„ LLMClient å®ä¾‹
- æ”¯æŒé€šè¿‡åç§°è·å–å®¢æˆ·ç«¯ï¼š`factory.getClient("deepseek")`
- ç®¡ç†æ‰€æœ‰æ¨¡å‹å®¢æˆ·ç«¯çš„ç”Ÿå‘½å‘¨æœŸ

### 3. å®¢æˆ·ç«¯å±‚ (Client Layer)

#### `OpenAICompatibleLLMClient.java`
- æ”¯æŒæ‰€æœ‰ OpenAI API å…¼å®¹çš„æä¾›å•†
- DeepSeekã€Kimiã€OpenAI éƒ½ä½¿ç”¨è¿™ä¸ªå®¢æˆ·ç«¯
- æ”¯æŒåŸç”Ÿ Function Calling

#### `OllamaLLMClient.java`
- ä¸“é—¨ç”¨äº Ollama æœ¬åœ°æ¨¡å‹
- ä½¿ç”¨ PromptTemplate æ ¼å¼åŒ–æ¶ˆæ¯ï¼ˆé€‚é…å°æ¨¡å‹ï¼‰

### 4. æœåŠ¡å±‚ (Service Layer)

#### `LLMService.java` (æ¥å£)
ç»Ÿä¸€çš„ LLM äº¤äº’æ¥å£ï¼š
```java
public interface LLMService {
    Message.AssistantMessage chat(List<Message> messages);
    Message.AssistantMessage chatWithModel(String modelName, List<Message> messages);
    Set<String> getAvailableModels();
}
```

#### `MultiModelLLMService.java` (å®ç°)
- æ”¯æŒåŠ¨æ€é€‰æ‹©æ¨¡å‹
- è‡ªåŠ¨é™çº§ç­–ç•¥ï¼ˆä¸»æ¨¡å‹å¤±è´¥æ—¶åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼‰
- è¶…æ—¶æ§åˆ¶ï¼ˆä½¿ç”¨è™šæ‹Ÿçº¿ç¨‹ï¼‰
- è°ƒç”¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆæˆåŠŸç‡ã€å¹³å‡å“åº”æ—¶é—´ï¼‰

### 5. æ³¨è§£å±‚ (Annotation Layer)

#### `@UseModel` æ³¨è§£
```java
@UseModel("deepseek")
public String generateCode(String prompt) {
    // è¿™ä¸ªæ–¹æ³•ä¼šä½¿ç”¨ DeepSeek æ¨¡å‹
}
```

#### `ModelSelectionAspect.java`
- AOP åˆ‡é¢ï¼Œæ‹¦æˆª @UseModel æ³¨è§£
- ä½¿ç”¨ ThreadLocal å­˜å‚¨æ¨¡å‹é€‰æ‹©
- è‡ªåŠ¨æ¸…ç†ä¸Šä¸‹æ–‡

## ğŸ¯ ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1: ä½¿ç”¨é»˜è®¤æ¨¡å‹
```java
@Autowired
private LLMService llmService;

public void example1() {
    List<Message> messages = List.of(
        new Message.UserMessage("Hello!")
    );
    Message.AssistantMessage response = llmService.chat(messages);
}
```

### æ–¹å¼ 2: ä½¿ç”¨ @UseModel æ³¨è§£
```java
@Service
public class MyService {
    @Autowired
    private LLMService llmService;

    @UseModel("deepseek")
    public String generateCode() {
        // è‡ªåŠ¨ä½¿ç”¨ DeepSeek æ¨¡å‹
        return llmService.chat(messages).content();
    }

    @UseModel("kimi")
    public String analyzeText() {
        // è‡ªåŠ¨ä½¿ç”¨ Kimi æ¨¡å‹
        return llmService.chat(messages).content();
    }
}
```

### æ–¹å¼ 3: åŠ¨æ€é€‰æ‹©æ¨¡å‹
```java
public void example3() {
    String modelName = "deepseek"; // è¿è¡Œæ—¶å†³å®š
    Message.AssistantMessage response =
        llmService.chatWithModel(modelName, messages);
}
```

### æ–¹å¼ 4: æŸ¥çœ‹å¯ç”¨æ¨¡å‹
```java
Set<String> models = llmService.getAvailableModels();
// è¾“å‡º: [ollama, deepseek, kimi]
```

## ğŸ”„ é™çº§ç­–ç•¥

å½“ä¸»æ¨¡å‹å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼š

```yaml
fallback:
  enabled: true
  order: [ollama, deepseek, kimi]
```

æ‰§è¡Œæµç¨‹ï¼š
1. ä¼˜å…ˆä½¿ç”¨ Ollamaï¼ˆæœ¬åœ°æ¨¡å‹ï¼Œå¿«é€Ÿï¼‰
2. å¦‚æœ Ollama è¶…æ—¶æˆ–å¤±è´¥ï¼Œåˆ‡æ¢åˆ° DeepSeek
3. å¦‚æœ DeepSeek ä¹Ÿå¤±è´¥ï¼Œåˆ‡æ¢åˆ° Kimi
4. æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

## ğŸ“Š ç›‘æ§ç»Ÿè®¡

```java
@Autowired
private MultiModelLLMService llmService;

public void showStats() {
    var stats = llmService.getStats();
    stats.forEach((model, stat) -> {
        System.out.printf("Model: %s, Success: %d, Failure: %d, Avg: %.2fms%n",
            model,
            stat.getSuccessCount(),
            stat.getFailureCount(),
            stat.getAverageDurationMs());
    });
}
```

## ğŸ¨ è®¾è®¡æ¨¡å¼

1. **å·¥å‚æ¨¡å¼**: `ModelProviderFactory` åˆ›å»ºä¸åŒçš„å®¢æˆ·ç«¯
2. **ç­–ç•¥æ¨¡å¼**: ä¸åŒçš„ LLMClient å®ç°ä¸åŒçš„è°ƒç”¨ç­–ç•¥
3. **æ¨¡æ¿æ–¹æ³•æ¨¡å¼**: `PromptTemplate` æ ¼å¼åŒ–æ¶ˆæ¯
4. **AOP**: `@UseModel` æ³¨è§£å®ç°å£°æ˜å¼æ¨¡å‹é€‰æ‹©
5. **é™çº§æ¨¡å¼**: è‡ªåŠ¨æ•…éšœè½¬ç§»

## ğŸš€ ä¸‹ä¸€æ­¥

1. **é…ç½®ç¯å¢ƒå˜é‡**:
```bash
export DEEPSEEK_API_KEY="your-deepseek-key"
export KIMI_API_KEY="your-kimi-key"
```

2. **å¯åŠ¨åº”ç”¨**:
```bash
mvn spring-boot:run
```

3. **æµ‹è¯•å¤šæ¨¡å‹**:
æŸ¥çœ‹ `MultiModelExample.java` ä¸­çš„ç¤ºä¾‹ä»£ç 

## ğŸ“ æ–°å¢æ–‡ä»¶æ¸…å•

```
src/main/java/com/nanobot/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ MultiModelProperties.java      # å¤šæ¨¡å‹é…ç½®å±æ€§
â”‚   â””â”€â”€ MultiModelConfig.java          # é…ç½®ç±»
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ LLMService.java            # ç»Ÿä¸€æœåŠ¡æ¥å£
â”‚   â”‚   â””â”€â”€ MultiModelLLMService.java  # å¤šæ¨¡å‹æœåŠ¡å®ç°
â”‚   â”œâ”€â”€ factory/
â”‚   â”‚   â””â”€â”€ ModelProviderFactory.java  # æ¨¡å‹å·¥å‚
â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â””â”€â”€ OpenAICompatibleLLMClient.java  # OpenAI å…¼å®¹å®¢æˆ·ç«¯
â”‚   â””â”€â”€ annotation/
â”‚       â”œâ”€â”€ UseModel.java              # æ¨¡å‹é€‰æ‹©æ³¨è§£
â”‚       â””â”€â”€ ModelSelectionAspect.java  # AOP åˆ‡é¢
â”œâ”€â”€ example/
â”‚   â””â”€â”€ MultiModelExample.java         # ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ resources/
    â””â”€â”€ application.yml                # YAML é…ç½®æ–‡ä»¶
```

æ¶æ„é‡æ„å®Œæˆï¼ä½ ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªä¼ä¸šçº§çš„å¤šæ¨¡å‹ LLM äº¤äº’å±‚ã€‚
